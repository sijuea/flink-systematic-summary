### **1. 概念**
*   **堆内内存（On-Heap Memory）**
    - 由 JVM 管理，参与 GC；
    - 包含：任务线程栈、操作符数据缓存、JVM 框架逻辑；
    - 配置示例：`taskmanager.memory.task.heap.size`；
    - 易受 GC 影响，适合小状态低延迟任务。
* **堆外内存（Off-Heap Memory）**
    - 由 native 层直接申请，JVM 不直接管理；
    - 包含：RocksDB、Network Buffer、Managed Memory（在 off-heap 模式下）；
    - 配置示例：`taskmanager.memory.task.off-heap.size`；
    - 高性能、无 GC 开销，适合大状态任务。
* **内存区域划分**
    
    Flink 在 TaskManager 中将总内存划分为：
    
    * **Framework Memory**（堆/堆外）：Flink 运行基础服务；
    * **Task Memory**（堆/堆外）：各算子执行逻辑；
    * **Managed Memory**：统一管理，用于 RocksDB、Sort、Join 等组件；
    * **Network Memory**：用于 shuffle 传输；
    * **JVM Overhead + Metaspace**：用于 JVM 自身管理。
    
    **补充说明：堆内 vs 堆外 是对内存物理分配机制的核心划分，而如 Managed Memory / Task Memory 是按使用逻辑划分，二者正交。**
    
* **内存模型类型**
    - **静态模式（Legacy）**：用户显式分配每部分内存；
    - **统一内存模型（Unified）**：Flink 自动根据总内存按比例分配各部分；
    - 推荐统一模型，简化配置，避免冲突。
* **资源共享与隔离**
    - 一个 TaskManager 内的多个 Slot 共享绝大多数内存（如网络、托管内存）；
    - 每个 Task 的使用被逻辑隔离，防止资源抢占。
### **2.原理：**
* **启动与内存预算：【提前约定内存使用量】**
    - Flink 启动时，根据配置项（如 `taskmanager.memory.process.size`）将内存划分为堆内、堆外、Managed 等多个区域；
    - 各区域在启动 TaskExecutor 时一次性初始化完成，形成内存预算边界，禁止动态突破。
* **Slot 层级资源复用与隔离：【申请slot】**
    - TaskManager 的内存按 Slot 分配，每个 Slot 独享其内存配额；
    - TaskManager 中每个 Slot 可运行多个算子 Chain，共享内存资源；
    - 物理隔离依赖于 Managed/Network Memory 的独立划分逻辑，Slot 本身逻辑隔离，不提供物理隔离。
* **Task 启动：【申请内存，准备运行】**
    - Task 启动时，根据算子需求向对应内存池申请资源：
        - 向 `MemoryManager` 申请 Managed Memory 页数（供 RocksDB、排序算子使用）；
        - 向 `NetworkBufferPool` 申请网络缓冲区（用于 shuffle 数据传输）；
        - 堆内存和系统内存由 JVM 和 TaskManager 管理；
        - 内存与 Task 生命周期绑定：Task 结束时，内存自动回收。
* **缓冲区申请流程：【申请数据流动的缓存区】**

     - Shuffle 数据传输使用 `NetworkBufferPool` 管理 buffer；
    - Flink 网络通信时，从全局 `NetworkBufferPool` 申请 buffer（相当于“水桶”）：
        - 每个数据交换通道（Channel）维护自己的 `LocalBufferPool`，从全局池借 buffer；
        - 数据生产者（上游 Task）填满缓冲区后，写入阻塞，等待消费者（下游 Task）消费后释放；
        - 若无法申请到足够 buffer，会阻塞数据传输或触发 backpressure；
    - Buffer 默认使用 Direct Memory，通过 `taskmanager.memory.network.*` 控制其容量。
* **反压信号传递机制：**    
    **触发条件**：
    
    - 下游任务 B 无法及时消费数据 → B 的 InputChannel buffer 被占满
    - A 无法再将数据 push 给 B → A 的 Output buffer 写入阻塞
    - A 的线程被阻塞或等待 → Backpressure 向上游传递
    - 所以下游任务B为当前执行任务得瓶颈
    - 此机制是确保 Flink 高通量系统中“数据不丢、不爆内存”的关键。
  
    **可观测路径**：
    - Web UI 显示红色 “backpressure”
        - Operator Metrics:
            - `BackPressuredTimeMsPerSecond`
            - `BusyTimeMsPerSecond`
* **统一内存模型：【各模块公用内存】**
    - Flink 算子（如 RocksDB、Sort、Join）通过注解声明对内存的用途（`@ManagedMemoryUseCase`）；
    - Flink 自动根据权重或算子类型分配 Managed 内存资源，替代手动调节。
* **任务结束：【释放内存，清理资源】**
    - Task 执行完成后，调用释放方法：
        - `MemoryManager` 归还 Managed Memory；
        - `NetworkBufferPool` 释放网络缓冲区；
        - 关闭 RocksDB 等 native 资源，确保 native 内存释放；
### **3.实现：**
* **关键类实现结构**
    - **`MemoryManager`**：负责 Managed Memory 的分配与释放；
    - **`NetworkBufferPool`**：负责网络缓冲的内存池分配；
    - **`TaskExecutorMemoryConfiguration`**：配置解析与预算入口；
    - **`TaskSlotTableImpl`**：实现 slot 的内存绑定与调度。
* **内存分配流程**
    - TaskManager 启动时读取配置；
    - 初始化各类内存区域；
    - 每个 Task 启动时注册内存需求；
    - 执行完毕自动归还 memory segment。
* **内存配置项解析**
    - `taskmanager.memory.process.size`：总内存；
    - `taskmanager.memory.managed.fraction`：Managed 内存占比；
    - `taskmanager.memory.network.min/max`：网络内存范围；
    - `taskmanager.memory.framework.heap.size`：系统内存分配。
* **指标系统对接**
    
    内存使用情况可通过：
    
    - Flink Web UI；
    - Prometheus 监控：如 `Status.JVM.Memory.HeapUsed`;
    - JMX 或 logs 记录内存分布详情。
### **4.场景：**
- **状态后端场景（State Backend）**
    - **RocksDBStateBackend** 使用 **Off-Heap（Direct Memory）**，由 native 层控制，不受 JVM GC 管控；
    - 建议开启 `state.backend.rocksdb.memory.managed=true`，使其接入 Flink 的统一内存模型（Managed Memory）；
    - 使用 RocksDB 时需限制 native 内存总量（例如通过 `taskmanager.memory.managed.size`）防止 OOM；
    - **EmbeddedRocksDBStateBackend** 支持与 `MemoryManager` 更紧密整合，在流场景中使用广泛。
- **流处理数据缓存**
    - Operator Chain 中的算子使用堆内缓存数据（如 Flink CEP、Window 聚合）；
    - 若 `Task Heap Memory` 不足，容易触发频繁 GC，影响低延迟处理。
- **Shuffle 网络传输**
    - TaskManager 间 shuffle、rebalance、keyBy 操作依赖 Network Buffer；
    - Network Memory 分配不足会导致数据阻塞、backpressure，加剧系统延迟。
### **5.常见问题**
- **Q：OOM（OutOfMemoryError）**
    > **A：**
    原因包括：Heap 内存不足、Managed Memory 超限、Direct Memory 泄漏；
    对策：检查内存预算配置，启用监控，合理拆分 job。
    > 
- **Q：Network Backpressure**
    
    > **A:**
    网络缓冲区耗尽导致传输阻塞；
    可通过增加 Network Buffer 或减小每 buffer size 解决。
    或者修改算子逻辑，提高数据处理速度
    > 
- **Q：Shuffle 死锁或失败**
    
    > **A:**
    网络缓冲分配不足，导致 shuffle 算子等待；
    建议提升 `taskmanager.network.memory.min` 设置值。
    > 
